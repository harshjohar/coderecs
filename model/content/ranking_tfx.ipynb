{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjUA6S30k52h"
   },
   "source": [
    "##### Copyright 2022 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:03.665121Z",
     "iopub.status.busy": "2022-03-30T11:15:03.664861Z",
     "iopub.status.idle": "2022-03-30T11:15:03.669176Z",
     "shell.execute_reply": "2022-03-30T11:15:03.668492Z"
    },
    "id": "SpNWyqewk8fE"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x1ypzczQCwy"
   },
   "source": [
    "# Using TensorFlow Recommenders with TFX\n",
    "\n",
    "***A tutorial to train a TensorFlow Recommenders ranking model as a [TFX pipeline](https://www.tensorflow.org/tfx).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU9YYythm0dx"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/ranking_tfx\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/ranking_tfx.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/ranking_tfx.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/ranking_tfx.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VuwrlnvQJ5k"
   },
   "source": [
    "In this notebook-based tutorial, we will create and run a [TFX pipeline](https://www.tensorflow.org/tfx)\n",
    "to train a ranking model to predict movie ratings using TensorFlow Recommenders (TFRS).\n",
    "The pipeline will consist of three essential TFX components: ExampleGen,\n",
    "Trainer and Pusher. The pipeline includes the most minimal ML workflow like\n",
    "importing data, training a model and exporting the trained TFRS ranking model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fmgi8ZvQkScg"
   },
   "source": [
    "## Set Up\n",
    "We first need to install the TFX Python package and download\n",
    "the dataset which we will use for our model.\n",
    "\n",
    "### Upgrade Pip\n",
    "\n",
    "To avoid upgrading Pip in a system when running locally,\n",
    "check to make sure that we are running in Colab.\n",
    "Local systems can of course be upgraded separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:03.672628Z",
     "iopub.status.busy": "2022-03-30T11:15:03.672128Z",
     "iopub.status.idle": "2022-03-30T11:15:03.678415Z",
     "shell.execute_reply": "2022-03-30T11:15:03.677872Z"
    },
    "id": "as4OTe2ukSqm"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  %pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZOYTt1RW4TK"
   },
   "source": [
    "### Install TFX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:03.681563Z",
     "iopub.status.busy": "2022-03-30T11:15:03.680940Z",
     "iopub.status.idle": "2022-03-30T11:15:10.423670Z",
     "shell.execute_reply": "2022-03-30T11:15:10.422877Z"
    },
    "id": "iyQtljP-qPHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tfx\n",
      "  Using cached tfx-1.8.0-py3-none-any.whl (2.5 MB)\n",
      "Collecting portpicker<2,>=1.3.1\n",
      "  Using cached portpicker-1.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting packaging<21,>=20\n",
      "  Using cached packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "Collecting pyyaml<6,>=3.12\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-macosx_10_9_x86_64.whl (259 kB)\n",
      "Collecting tensorflow-hub<0.13,>=0.9.0\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting attrs<21,>=19.3.0\n",
      "  Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Collecting tfx-bsl<1.9.0,>=1.8.0\n",
      "  Using cached tfx_bsl-1.8.0-cp39-cp39-macosx_10_14_x86_64.whl (20.5 MB)\n",
      "Collecting tensorflow-data-validation<1.9.0,>=1.8.0\n",
      "  Using cached tensorflow_data_validation-1.8.0-cp39-cp39-macosx_10_14_x86_64.whl (1.5 MB)\n",
      "Collecting docker<5,>=4.1\n",
      "  Using cached docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
      "Collecting ml-pipelines-sdk==1.8.0\n",
      "  Using cached ml_pipelines_sdk-1.8.0-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: protobuf<4,>=3.13 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from tfx) (3.19.1)\n",
      "Collecting google-cloud-bigquery<3,>=2.26.0\n",
      "  Using cached google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "Collecting pyarrow<6,>=1\n",
      "  Using cached pyarrow-5.0.0-cp39-cp39-macosx_10_13_x86_64.whl (17.6 MB)\n",
      "Collecting google-apitools<1,>=0.5\n",
      "  Using cached google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
      "Collecting google-cloud-aiplatform<2,>=1.6.2\n",
      "  Using cached google_cloud_aiplatform-1.15.0-py2.py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2<4,>=2.7.3 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from tfx) (2.11.3)\n",
      "Collecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5\n",
      "  Using cached tensorflow-2.8.2-cp39-cp39-macosx_10_14_x86_64.whl (217.8 MB)\n",
      "Collecting tensorflow-transform<1.9.0,>=1.8.0\n",
      "  Using cached tensorflow_transform-1.8.0-py3-none-any.whl (435 kB)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from tfx) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.28.1 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from tfx) (1.42.0)\n",
      "Collecting keras-tuner<2,>=1.0.4\n",
      "  Using cached keras_tuner-1.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting kubernetes<13,>=10.0.1\n",
      "  Using cached kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting click<8,>=7\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting apache-beam[gcp]<3,>=2.38\n",
      "  Using cached apache_beam-2.40.0-cp39-cp39-macosx_10_9_x86_64.whl (4.7 MB)\n",
      "Collecting google-api-python-client<2,>=1.8\n",
      "  Using cached google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Collecting ml-metadata<1.9.0,>=1.8.0\n",
      "  Using cached ml_metadata-1.8.0-cp39-cp39-macosx_10_14_x86_64.whl (18.9 MB)\n",
      "Requirement already satisfied: numpy<2,>=1.16 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from tfx) (1.21.5)\n",
      "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15\n",
      "  Using cached tensorflow_serving_api-2.9.1-py2.py3-none-any.whl (37 kB)\n",
      "Collecting tensorflow-model-analysis<0.40,>=0.39.0\n",
      "  Using cached tensorflow_model_analysis-0.39.0-py3-none-any.whl (1.8 MB)\n",
      "Collecting httplib2<0.21.0,>=0.8\n",
      "  Using cached httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "Collecting fastavro<2,>=0.23.6\n",
      "  Using cached fastavro-1.5.2-cp39-cp39-macosx_10_15_x86_64.whl (506 kB)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Using cached pymongo-3.12.3-cp39-cp39-macosx_10_9_x86_64.whl (395 kB)\n",
      "Requirement already satisfied: pytz>=2018.3 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.38->tfx) (2021.3)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.38->tfx) (2.8.2)\n",
      "Collecting cloudpickle<3,>=2.1.0\n",
      "  Using cached cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Using cached dill-0.3.1.1.tar.gz (151 kB)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Using cached crcmod-1.7.tar.gz (89 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Using cached hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting proto-plus<2,>=1.7.1\n",
      "  Using cached proto_plus-1.20.6-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.38->tfx) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.38->tfx) (4.1.1)\n",
      "Collecting orjson<4.0\n",
      "  Using cached orjson-3.7.7-cp39-cp39-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (447 kB)\n",
      "Collecting google-cloud-bigquery-storage>=2.6.3\n",
      "  Using cached google_cloud_bigquery_storage-2.14.1-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Using cached google_cloud_bigtable-1.7.2-py2.py3-none-any.whl (267 kB)\n",
      "Collecting google-cloud-vision<2,>=0.38.0\n",
      "  Using cached google_cloud_vision-1.0.2-py2.py3-none-any.whl (435 kB)\n",
      "Collecting google-auth-httplib2<0.2.0,>=0.1.0\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-cloud-datastore<2,>=1.8.0\n",
      "  Using cached google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.38->tfx) (1.7.1)\n",
      "Collecting google-apitools<1,>=0.5\n",
      "  Using cached google-apitools-0.5.31.tar.gz (173 kB)\n",
      "Collecting google-cloud-language<2,>=1.3.0\n",
      "  Using cached google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n",
      "Collecting grpcio-gcp<1,>=0.2.2\n",
      "  Using cached grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
      "Collecting google-cloud-recommendations-ai<=0.2.0,>=0.1.0\n",
      "  Using cached google_cloud_recommendations_ai-0.2.0-py2.py3-none-any.whl (180 kB)\n",
      "Collecting google-cloud-pubsublite<2,>=1.2.0\n",
      "  Using cached google_cloud_pubsublite-1.4.2-py2.py3-none-any.whl (265 kB)\n",
      "Collecting google-cloud-spanner<2,>=1.13.0\n",
      "  Using cached google_cloud_spanner-1.19.3-py2.py3-none-any.whl (255 kB)\n",
      "Collecting google-cloud-pubsub<3,>=2.1.0\n",
      "  Using cached google_cloud_pubsub-2.13.3-py2.py3-none-any.whl (234 kB)\n",
      "Collecting google-cloud-dlp<4,>=3.0.0\n",
      "  Using cached google_cloud_dlp-3.7.1-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.38->tfx) (4.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.38->tfx) (1.33.0)\n",
      "Collecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Using cached google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from docker<5,>=4.1->tfx) (1.16.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from docker<5,>=4.1->tfx) (0.58.0)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Using cached uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from google-api-python-client<2,>=1.8->tfx) (1.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.8->tfx) (1.53.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.8->tfx) (61.2.0)\n",
      "Collecting fasteners>=0.14\n",
      "  Using cached fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
      "Collecting oauth2client>=1.4.12\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.38->tfx) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.38->tfx) (4.7.2)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Using cached google_cloud_resource_manager-1.6.0-py2.py3-none-any.whl (231 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Using cached google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Using cached google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Using cached googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Using cached grpcio_status-1.47.0-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /Users/harshjohar/opt/anaconda3/lib/python3.9/site-packages (from google-cloud-bigquery<3,>=2.26.0->tfx) (1.3.1)\n",
      "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Using cached grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting google-cloud-core<2,>=0.28.1\n",
      "  Using cached google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
      "Collecting overrides<7.0.0,>=6.0.1\n",
      "  Using cached overrides-6.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting google-cloud-recommendations-ai<=0.2.0,>=0.1.0\n",
      "  Using cached google_cloud_recommendations_ai-0.1.0-py2.py3-none-any.whl (99 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-pubsublite to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-pubsublite<2,>=1.2.0\n",
      "  Using cached google_cloud_pubsublite-1.4.1-py2.py3-none-any.whl (265 kB)\n",
      "  Using cached google_cloud_pubsublite-1.4.0-py2.py3-none-any.whl (261 kB)\n",
      "  Using cached google_cloud_pubsublite-1.3.0-py2.py3-none-any.whl (255 kB)\n",
      "  Using cached google_cloud_pubsublite-1.2.0-py2.py3-none-any.whl (252 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-pubsub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-pubsub<3,>=2.1.0\n",
      "  Using cached google_cloud_pubsub-2.13.2-py2.py3-none-any.whl (234 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-pubsublite to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached google_cloud_pubsub-2.13.1-py2.py3-none-any.whl (234 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Using cached google_cloud_pubsub-2.13.0-py2.py3-none-any.whl (234 kB)\n",
      "  Using cached google_cloud_pubsub-2.12.1-py2.py3-none-any.whl (233 kB)\n",
      "  Using cached google_cloud_pubsub-2.12.0-py2.py3-none-any.whl (232 kB)\n",
      "  Using cached google_cloud_pubsub-2.11.1-py2.py3-none-any.whl (232 kB)\n",
      "  Using cached google_cloud_pubsub-2.11.0-py2.py3-none-any.whl (232 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-pubsub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached google_cloud_pubsub-2.10.0-py2.py3-none-any.whl (231 kB)\n",
      "  Using cached google_cloud_pubsub-2.9.0-py2.py3-none-any.whl (219 kB)\n",
      "Collecting libcst>=0.3.10\n",
      "  Using cached libcst-0.4.7-cp39-cp39-macosx_10_9_x86_64.whl (1.8 MB)\n",
      "Collecting google-cloud-pubsub<3,>=2.1.0\n",
      "  Using cached google_cloud_pubsub-2.8.0-py2.py3-none-any.whl (217 kB)\n",
      "  Using cached google_cloud_pubsub-2.7.1-py2.py3-none-any.whl (217 kB)\n",
      "  Using cached google_cloud_pubsub-2.7.0-py2.py3-none-any.whl (217 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Using cached google_cloud_pubsub-2.6.1-py2.py3-none-any.whl (212 kB)\n",
      "  Using cached google_cloud_pubsub-2.6.0-py2.py3-none-any.whl (212 kB)\n",
      "  Using cached google_cloud_pubsub-2.5.0-py2.py3-none-any.whl (211 kB)\n",
      "  Using cached google_cloud_pubsub-2.4.2-py2.py3-none-any.whl (212 kB)\n",
      "  Using cached google_cloud_pubsub-2.4.1-py2.py3-none-any.whl (212 kB)\n",
      "  Using cached google_cloud_pubsub-2.3.0-py2.py3-none-any.whl (210 kB)\n",
      "  Using cached google_cloud_pubsub-2.2.0-py2.py3-none-any.whl (179 kB)\n",
      "  Using cached google_cloud_pubsub-2.1.0-py2.py3-none-any.whl (177 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-language to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-language<2,>=1.3.0\n",
      "  Using cached google_cloud_language-1.3.1-py2.py3-none-any.whl (83 kB)\n",
      "  Using cached google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-dlp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-dlp<4,>=3.0.0\n",
      "  Using cached google_cloud_dlp-3.7.0-py2.py3-none-any.whl (118 kB)\n",
      "  Using cached google_cloud_dlp-3.6.2-py2.py3-none-any.whl (113 kB)\n",
      "  Using cached google_cloud_dlp-3.6.1-py2.py3-none-any.whl (113 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-language to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached google_cloud_dlp-3.6.0-py2.py3-none-any.whl (111 kB)\n",
      "  Using cached google_cloud_dlp-3.5.0-py2.py3-none-any.whl (110 kB)\n",
      "  Using cached google_cloud_dlp-3.4.0-py2.py3-none-any.whl (110 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Using cached google_cloud_dlp-3.3.1-py2.py3-none-any.whl (110 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-dlp to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached google_cloud_dlp-3.3.0-py2.py3-none-any.whl (109 kB)\n",
      "  Using cached google_cloud_dlp-3.2.4-py2.py3-none-any.whl (109 kB)\n",
      "  Using cached google_cloud_dlp-3.2.3-py2.py3-none-any.whl (109 kB)\n",
      "  Using cached google_cloud_dlp-3.2.2-py2.py3-none-any.whl (109 kB)\n",
      "  Using cached google_cloud_dlp-3.2.1-py2.py3-none-any.whl (109 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Using cached google_cloud_dlp-3.2.0-py2.py3-none-any.whl (109 kB)\n",
      "  Using cached google_cloud_dlp-3.1.1-py2.py3-none-any.whl (109 kB)\n",
      "  Using cached google_cloud_dlp-3.1.0-py2.py3-none-any.whl (153 kB)\n",
      "  Using cached google_cloud_dlp-3.0.1-py2.py3-none-any.whl (150 kB)\n",
      "  Using cached google_cloud_dlp-3.0.0-py2.py3-none-any.whl (150 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-datastore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-datastore<2,>=1.8.0\n",
      "  Using cached google_cloud_datastore-1.15.4-py2.py3-none-any.whl (134 kB)\n",
      "  Using cached google_cloud_datastore-1.15.3-py2.py3-none-any.whl (134 kB)\n",
      "  Using cached google_cloud_datastore-1.15.2-py2.py3-none-any.whl (134 kB)\n",
      "  Using cached google_cloud_datastore-1.15.1-py2.py3-none-any.whl (133 kB)\n",
      "  Using cached google_cloud_datastore-1.15.0-py2.py3-none-any.whl (132 kB)\n",
      "  Using cached google_cloud_datastore-1.14.0-py2.py3-none-any.whl (131 kB)\n",
      "  Using cached google_cloud_datastore-1.13.2-py2.py3-none-any.whl (131 kB)\n",
      "  Using cached google_cloud_datastore-1.13.1-py2.py3-none-any.whl (131 kB)\n",
      "  Using cached google_cloud_datastore-1.13.0-py2.py3-none-any.whl (105 kB)\n",
      "  Using cached google_cloud_datastore-1.12.0-py2.py3-none-any.whl (97 kB)\n",
      "  Using cached google_cloud_datastore-1.11.0-py2.py3-none-any.whl (97 kB)\n",
      "  Using cached google_cloud_datastore-1.10.0-py2.py3-none-any.whl (97 kB)\n",
      "  Using cached google_cloud_datastore-1.9.0-py2.py3-none-any.whl (97 kB)\n",
      "  Using cached google_cloud_datastore-1.8.0-py2.py3-none-any.whl (96 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-core<2,>=0.28.1\n",
      "  Using cached google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "  Using cached google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n",
      "  Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
      "  Using cached google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
      "  Using cached google_cloud_core-1.4.4-py2.py3-none-any.whl (27 kB)\n",
      "  Using cached google_cloud_core-1.4.3-py2.py3-none-any.whl (27 kB)\n",
      "  Using cached google_cloud_core-1.4.2-py2.py3-none-any.whl (26 kB)\n",
      "  Using cached google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-bigtable to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Using cached google_cloud_bigtable-1.7.1-py2.py3-none-any.whl (267 kB)\n",
      "  Using cached google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
      "  Using cached google_cloud_bigtable-1.6.1-py2.py3-none-any.whl (267 kB)\n",
      "  Using cached google_cloud_bigtable-1.6.0-py2.py3-none-any.whl (267 kB)\n",
      "  Using cached google_cloud_bigtable-1.5.1-py2.py3-none-any.whl (266 kB)\n",
      "  Using cached google_cloud_bigtable-1.5.0-py2.py3-none-any.whl (266 kB)\n",
      "  Using cached google_cloud_bigtable-1.4.0-py2.py3-none-any.whl (265 kB)\n",
      "  Using cached google_cloud_bigtable-1.3.0-py2.py3-none-any.whl (259 kB)\n",
      "  Using cached google_cloud_bigtable-1.2.1-py2.py3-none-any.whl (234 kB)\n",
      "  Using cached google_cloud_bigtable-1.2.0-py2.py3-none-any.whl (234 kB)\n",
      "  Using cached google_cloud_bigtable-1.1.0-py2.py3-none-any.whl (234 kB)\n",
      "  Using cached google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232 kB)\n",
      "  Using cached google_cloud_bigtable-0.34.0-py2.py3-none-any.whl (232 kB)\n",
      "  Using cached google_cloud_bigtable-0.33.0-py2.py3-none-any.whl (230 kB)\n",
      "  Using cached google_cloud_bigtable-0.32.2-py2.py3-none-any.whl (156 kB)\n",
      "  Using cached google_cloud_bigtable-0.32.1-py2.py3-none-any.whl (156 kB)\n",
      "  Using cached google_cloud_bigtable-0.32.0-py2.py3-none-any.whl (155 kB)\n",
      "  Using cached google_cloud_bigtable-0.31.1-py2.py3-none-any.whl (154 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-bigquery-storage to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-bigquery-storage>=2.6.3\n",
      "  Using cached google_cloud_bigquery_storage-2.14.0-py2.py3-none-any.whl (181 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-datastore to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tfx\n",
    "!pip install -U tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwT0nov5QO1M"
   },
   "source": [
    "### Did you restart the runtime?\n",
    "\n",
    "If you are using Google Colab, the first time that you run\n",
    "the cell above, you must restart the runtime by clicking\n",
    "above \"RESTART RUNTIME\" button or using \"Runtime > Restart\n",
    "runtime ...\" menu. This is because of the way that Colab\n",
    "loads packages.\n",
    "\n",
    "Before we define the pipeline, we need to write the model code for the\n",
    "Trainer component and save it in a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDnPgN8UJtzN"
   },
   "source": [
    "Check the TensorFlow and TFX versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:10.428411Z",
     "iopub.status.busy": "2022-03-30T11:15:10.427809Z",
     "iopub.status.idle": "2022-03-30T11:15:14.989299Z",
     "shell.execute_reply": "2022-03-30T11:15:14.988701Z"
    },
    "id": "6jh7vKSRqPHb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDtLdSkvqPHe"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "There are some variables used to define a pipeline. You can customize these\n",
    "variables as you want. By default all output from the pipeline will be\n",
    "generated under the current directory. Instead of using the SchemaGen component to generate a schema, for this\n",
    "tutorial we will create a hardcoded schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:14.993088Z",
     "iopub.status.busy": "2022-03-30T11:15:14.992631Z",
     "iopub.status.idle": "2022-03-30T11:15:14.997768Z",
     "shell.execute_reply": "2022-03-30T11:15:14.997178Z"
    },
    "id": "EcUseqJaE2XN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_NAME = 'TFRS-ranking'\n",
    "\n",
    "# Directory where MovieLens 100K rating data lives\n",
    "DATA_ROOT = os.path.join('data', PIPELINE_NAME)\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F2SRwRLSYGa"
   },
   "source": [
    "### Prepare example data\n",
    "Since TFX does not currently support TensorFlow Datasets API, we will download the MovieLens 100K dataset manually for use in our TFX pipeline. The dataset we\n",
    "are using is\n",
    "[MovieLens 100K Dataset](https://grouplens.org/datasets/movielens/100k/).\n",
    "\n",
    "There are four numeric features in this dataset:\n",
    "\n",
    "- userId\n",
    "- movieId\n",
    "- rating\n",
    "- timestamp\n",
    "\n",
    "We will build a ranking model which predicts the `rating` of the movies. We will not use the `timestamp` feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11J7XiCq6AFP"
   },
   "source": [
    "Because TFX ExampleGen reads inputs from a directory, we need to create a\n",
    "directory and copy dataset to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:15.001220Z",
     "iopub.status.busy": "2022-03-30T11:15:15.000779Z",
     "iopub.status.idle": "2022-03-30T11:15:18.769769Z",
     "shell.execute_reply": "2022-03-30T11:15:18.768745Z"
    },
    "id": "4fxMs6u86acP"
   },
   "outputs": [],
   "source": [
    "# !wget https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "# !mkdir -p {DATA_ROOT}\n",
    "# !unzip ml-100k.zip\n",
    "# !echo 'username,ID,rating' > {DATA_ROOT}/ratings.csv\n",
    "# !sed 's/\\t/,/g' ml-100k/u.data >> {DATA_ROOT}/ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASpoNmxKSQjI"
   },
   "source": [
    "Take a quick look at the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:18.774365Z",
     "iopub.status.busy": "2022-03-30T11:15:18.773811Z",
     "iopub.status.idle": "2022-03-30T11:15:18.897216Z",
     "shell.execute_reply": "2022-03-30T11:15:18.896505Z"
    },
    "id": "-eSz28UDSnlG"
   },
   "outputs": [],
   "source": [
    "!head {DATA_ROOT}/ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTtQNq1DdVvG"
   },
   "source": [
    "You should be able to see four values. For example, the first example means user '196' gives a rating of 3 to movie '242'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH6gizcpSwWV"
   },
   "source": [
    "## Create a pipeline\n",
    "\n",
    "TFX pipelines are defined using Python APIs. We will define a pipeline which\n",
    "consists of following three components.\n",
    "- CsvExampleGen: Reads in data files and convert them to TFX internal format\n",
    "for further processing. There are multiple\n",
    "[ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen)s for various\n",
    "formats. In this tutorial, we will use CsvExampleGen which takes CSV file input.\n",
    "- Trainer: Trains an ML model.\n",
    "[Trainer component](https://www.tensorflow.org/tfx/guide/trainer) requires a\n",
    "model definition code from users. You can use TensorFlow APIs to specify how to\n",
    "train a model and save it in a _saved_model_ format.\n",
    "- Pusher: Copies the trained model outside of the TFX pipeline.\n",
    "[Pusher component](https://www.tensorflow.org/tfx/guide/pusher) can be thought\n",
    "of an deployment process of the trained ML model.\n",
    "\n",
    "Before actually define the pipeline, we need to write a model code for the\n",
    "Trainer component first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOjDv93eS5xV"
   },
   "source": [
    "### Write model training code\n",
    "\n",
    "We will build a simple ranking model to predict movie ratings. This model training code will be saved to a separate file.\n",
    "\n",
    "In this tutorial we will use\n",
    "[Generic Trainer](https://www.tensorflow.org/tfx/guide/trainer#generic_trainer)\n",
    "of TFX which support Keras-based models. You need to write a Python file\n",
    "containing `run_fn` function, which is the entrypoint for the `Trainer`\n",
    "component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:18.901982Z",
     "iopub.status.busy": "2022-03-30T11:15:18.901404Z",
     "iopub.status.idle": "2022-03-30T11:15:18.905009Z",
     "shell.execute_reply": "2022-03-30T11:15:18.904303Z"
    },
    "id": "aES7Hv5QTDK3"
   },
   "outputs": [],
   "source": [
    "_trainer_module_file = 'tfrs_ranking_trainer.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFsQCOytiidq"
   },
   "source": [
    "The ranking model we use is almost exactly the same as in the [Basic Ranking](https://www.tensorflow.org/recommenders/examples/basic_ranking) tutorial. The only difference is that we use movie IDs instead of movie titles in the candidate tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:18.908494Z",
     "iopub.status.busy": "2022-03-30T11:15:18.908293Z",
     "iopub.status.idle": "2022-03-30T11:15:18.915317Z",
     "shell.execute_reply": "2022-03-30T11:15:18.914518Z"
    },
    "id": "Gnc67uQNTDfW"
   },
   "outputs": [],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "from typing import Dict, Text\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "_FEATURE_KEYS = ['user_id', 'ID']\n",
    "_LABEL_KEY = 'rating'\n",
    "\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "        for feature in _FEATURE_KEYS\n",
    "    }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "df = pd.read_csv('data/TFRS-ranking/ratings.csv')\n",
    "user_count = len(df['user_id'].unique())\n",
    "problem_count = len(df['ID'].unique())\n",
    "\n",
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 32\n",
    "\n",
    "    unique_user_ids = np.array(range(user_count)).astype(str)\n",
    "    unique_problem_ids = np.array(range(problem_count)).astype(str)\n",
    "\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='user_id', dtype=tf.int64),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_user_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for problems.\n",
    "    self.problem_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='ID', dtype=tf.int64),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_problem_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_problem_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.ratings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    user_id, problem_id = inputs\n",
    "\n",
    "    user_embedding = self.user_embeddings(user_id)\n",
    "    problem_embedding = self.problem_embeddings(problem_id)\n",
    "\n",
    "    return self.ratings(tf.concat([user_embedding, problem_embedding], axis=2))\n",
    "\n",
    "\n",
    "class problemlensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ranking_model: tf.keras.Model = RankingModel()\n",
    "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "    return self.ranking_model((features['user_id'], features['ID']))\n",
    "\n",
    "  def compute_loss(self,\n",
    "                   features: Dict[Text, tf.Tensor],\n",
    "                   training=False) -> tf.Tensor:\n",
    "\n",
    "    labels = features[1]\n",
    "    rating_predictions = self(features[0])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(labels=labels, predictions=rating_predictions)\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 256) -> tf.data.Dataset:\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  return problemlensModel()\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files, fn_args.data_accessor, schema, batch_size=8192)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files, fn_args.data_accessor, schema, batch_size=4096)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      epochs = 3,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  model.save(fn_args.serving_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blaw0rs-emEf"
   },
   "source": [
    "Now you have completed all preparation steps to build the TFX pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3OkNz3gTLwM"
   },
   "source": [
    "### Write a pipeline definition\n",
    "\n",
    "We define a function to create a TFX pipeline. A `Pipeline` object\n",
    "represents a TFX pipeline which can be run using one of pipeline\n",
    "orchestration systems that TFX supports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:18.918786Z",
     "iopub.status.busy": "2022-03-30T11:15:18.918398Z",
     "iopub.status.idle": "2022-03-30T11:15:18.927702Z",
     "shell.execute_reply": "2022-03-30T11:15:18.926965Z"
    },
    "id": "M49yYVNBTPd4"
   },
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a three component pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=12),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=24))\n",
    "\n",
    "  # Pushes the model to a filesystem destination.\n",
    "  pusher = tfx.components.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      push_destination=tfx.proto.PushDestination(\n",
    "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir)))\n",
    "\n",
    "  # Following three components will be included in the pipeline.\n",
    "  components = [\n",
    "      example_gen,\n",
    "      trainer,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJbq07THU2GV"
   },
   "source": [
    "## Run the pipeline\n",
    "\n",
    "TFX supports multiple orchestrators to run pipelines.\n",
    "In this tutorial we will use `LocalDagRunner` which is included in the TFX\n",
    "Python package and runs pipelines on local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mp0AkmrPdUb"
   },
   "source": [
    "Now we create a `LocalDagRunner` and pass a `Pipeline` object created from the\n",
    "function we already defined.\n",
    "\n",
    "The pipeline runs directly and you can see logs for the progress of the pipeline including ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:18.931155Z",
     "iopub.status.busy": "2022-03-30T11:15:18.930629Z",
     "iopub.status.idle": "2022-03-30T11:15:50.996929Z",
     "shell.execute_reply": "2022-03-30T11:15:50.996294Z"
    },
    "id": "fAtfOZTYWJu-"
   },
   "outputs": [],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      module_file=_trainer_module_file,\n",
    "      serving_model_dir=SERVING_MODEL_DIR,\n",
    "      metadata_path=METADATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppERq0Mj6xvW"
   },
   "source": [
    "You should see \"INFO:absl:Component Pusher is finished.\" at the end of the\n",
    "logs if the pipeline finished successfully. Because `Pusher` component is the\n",
    "last component of the pipeline.\n",
    "\n",
    "The pusher component pushes the trained model to the `SERVING_MODEL_DIR` which\n",
    "is the `serving_model/TFRS-ranking` directory if you did not change the\n",
    "variables in the previous steps. You can see the result from the file browser\n",
    "in the left-side panel in Colab, or using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:51.000648Z",
     "iopub.status.busy": "2022-03-30T11:15:51.000090Z",
     "iopub.status.idle": "2022-03-30T11:15:51.150519Z",
     "shell.execute_reply": "2022-03-30T11:15:51.149729Z"
    },
    "id": "NTHROkqX6yHx"
   },
   "outputs": [],
   "source": [
    "# List files in created model directory.\n",
    "!ls -R {SERVING_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8HQfT-ziids"
   },
   "source": [
    "Now we can test the ranking model by computing predictions for a user and a movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-30T11:15:51.155091Z",
     "iopub.status.busy": "2022-03-30T11:15:51.154530Z",
     "iopub.status.idle": "2022-03-30T11:15:52.202316Z",
     "shell.execute_reply": "2022-03-30T11:15:52.201647Z"
    },
    "id": "5EDMkz8Wiidt"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "# Load the latest model for testing\n",
    "loaded = tf.saved_model.load(max(glob.glob(os.path.join(SERVING_MODEL_DIR, '*/')), key=os.path.getmtime))\n",
    "print(loaded({'username': [['Senna_12']], 'ID': [[132332]]}).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08R8qvweThRf"
   },
   "source": [
    "This concludes the TensorFlow Recommenders + TFX tutorial."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DjUA6S30k52h"
   ],
   "name": "ranking_tfx.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e4a5fbdc22c15db47583869bb086555a28756c364b70cc7ebc12e02fcd6a49c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
